{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook we will walk through what the K-Nearest Neighbors algorithm is, and how to implement it with RAPIDS cuML for both classification and regression.\n",
    "\n",
    "# K-Nearest Neighbors with cuML\n",
    "K-Nearest Neighbors (KNN) is a simple way to determine the value of something by asking what the values of the K nearest things to it are.\n",
    "\n",
    "So if K=3, what are the 3 nearest things?\n",
    "\n",
    "![KNN classification, K=3](https://miro.medium.com/max/540/0*49s1xDlDKDsn55xa.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing KNN with cuML\n",
    "We'll first walk through KNN for classification (`KNeighborsClassifier`), then run through KNN for regression (`KNeighborsRegressor`).\n",
    "\n",
    "### Step 1: Start with a Dataset with Known Target Values\n",
    "\n",
    "In our case, we have 3 species of Iris flower (Iris setosa, Iris virginica and Iris versicolor) from Fischer's Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "\n",
    "df = cudf.read_csv('https://github.com/gumdropsteve/datasets/raw/master/iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.species.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 instances (flowers) of each species were recorded, making 150 total records (rows).\n",
    "\n",
    "![iris species](https://miro.medium.com/max/600/0*XWbAz8JSpDxsnc8d)\n",
    "\n",
    "4 features - the length and width of the sepals and petals -  were measured in centimeters from each flower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these 4 features, Fisher developed a linear discriminant model to distinguish the species from each other.\n",
    "\n",
    "#### Data Prep\n",
    "IRL, you may often need to identify clusters in your dataset (our data has already been clustered because we know the species of each recorded flower). This can be done a number of ways including PCA and K-Means clustering.\n",
    "\n",
    "Before we visualize the species clusters, however, we should use cuML's `train_test_split()` function to break the dataset into training and testing subsets. This will allow us to judge performance with real data the model has never seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.preprocessing.model_selection import train_test_split\n",
    "\n",
    "df = df.drop('species')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, 'target', train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, by converting the X_train cudf.DataFrame `.to_pandas()`, we can easily visualize species clusters through Matplotlib.\n",
    "\n",
    "In this `.plot()` the red dots are virginica instances, the purple dots are setosa, and those in between are versicolor. Setting `c=y_train.to_array()` gives the `cmap` context to differentiate between species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pandas().plot(kind='scatter', \n",
    "                         x='sepal_length', \n",
    "                         y='petal_width',\n",
    "                         c=y_train.to_array(), cmap=('rainbow'), sharex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **KNN for Classification**\n",
    "K-Nearest Neighbors Classifier is an instance-based learning technique, that keeps training samples around for prediction, rather than trying to learn a generalizable set of model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the data, we can create and `.fit()` (\"train\") cuML's KNeighborsClassifier model with `X_train` and `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Add New Data with Unknown Target Values\n",
    "It's time to add in our testing data. Let's scatter the `X_test` flowers in black on top  of the rainbow plot to better understand where they're coming from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('sepal_length'), plt.ylabel('petal_width')\n",
    "\n",
    "# scatter training dataset (rainbow) + colorbar\n",
    "plt.scatter(x=X_train['sepal_length'].to_pandas(), y=X_train['petal_width'].to_pandas(),\n",
    "            c=y_train.to_pandas(), cmap='rainbow')\n",
    "plt.colorbar()\n",
    "\n",
    "# scatter testing dataset (black)\n",
    "plt.scatter(x=X_test['sepal_length'].to_pandas(), y=X_test['petal_width'].to_pandas(),\n",
    "            color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's pretty easy to guess which test flowers are setosa, it's not so obvious for the virginica or the versicolor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Classify the New Data\n",
    "We can classify these new `X_test` flowers by looking at the species of the nearest recorded flowers from `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = knn.predict(X_test)\n",
    "\n",
    "results.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Results\n",
    "Making a `.copy()` of the `X_test` DataFrame, we can add `actual` and `predicted` columns from `y_test` and `results` values (respectively).\n",
    "\n",
    "This results DataFrame (`df`) will help us examine performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = X_test.copy()\n",
    "\n",
    "df['actual'] = y_test.values\n",
    "df['predicted'] = results.values\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many instances did we `.predict()` correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_correct = len(df.loc[df['actual']==df['predicted']])\n",
    "n_possible = len(df)\n",
    "\n",
    "print(f'{n_correct} / {n_possible} correct\\n{str(n_correct / n_possible * 100)[:5]}% accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize by adding the correctly predicted species to the rainbow map alongside `X_train` and the incorrectly predicted flowers in black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correct = df.loc[df['actual'] == df['predicted']]\n",
    "df_incorrect = df.loc[df['actual'] != df['predicted']]\n",
    "\n",
    "plt.xlabel('sepal_length'), plt.ylabel('petal_width')\n",
    "\n",
    "# plot training instances (rainbow) + colorbar\n",
    "plt.scatter(x=X_train['sepal_length'].to_pandas(), y=X_train['petal_width'].to_pandas(),\n",
    "            c=y_train.to_pandas(), cmap='rainbow')\n",
    "plt.colorbar()\n",
    "\n",
    "# plot correctly predicted instances (rainbow)\n",
    "plt.scatter(x=df_correct['sepal_length'].to_pandas(), y=df_correct['petal_width'].to_pandas(),\n",
    "            c=df_correct['actual'].to_pandas(), cmap='rainbow')\n",
    "\n",
    "# plot incorrectly predicted instances (black) (except: means 1 or less incorrect)\n",
    "try:\n",
    "    plt.scatter(x=df_incorrect['sepal_length'].to_pandas(), y=df_incorrect['petal_width'].to_pandas(), color='k')\n",
    "except:\n",
    "    plt.scatter(x=df_incorrect['sepal_length'], y=df_incorrect['petal_width'], color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **KNN for Regression**\n",
    "K-Nearest Neighbors Regressor is an instance-based learning technique that keeps training samples around for prediction, rather than trying to learn a generalizable set of model parameters. The K-Nearest Neighbors Regressor will compute the average of the labels for the k closest neighbors and use it as the label.\n",
    "\n",
    "Creating the model, fitting it with data, and making predictions is done the same here for regression as it just was for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = knn.predict(X_test)\n",
    "\n",
    "results.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the nature of regression, results are returned as floating point values. Notice some predictions might have neighbors of different species, leading the `KNeighborsRegressor` to predict values like 1.666667.\n",
    "\n",
    "#### Regressor Results\n",
    "Let's add back the actual (`y_test`) values along with the predicted (`results`) to a `.copy()` of the `X_test` DataFrame and show the last 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = X_test.copy()\n",
    "\n",
    "df['actual'] = y_test.values\n",
    "df['predicted'] = results.values\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many instances did we `.predict()` correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_correct = len(df.loc[df['actual']==df['predicted']])\n",
    "n_possible = len(df)\n",
    "\n",
    "print(f'{n_correct} / {n_possible} correct\\n{str(n_correct / n_possible * 100)[:5]}% accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize by adding the correctly predicted species to the rainbow map alongside `X_train` and the incorrectly predicted flowers in black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correct = df.loc[df['actual'] == df['predicted']]\n",
    "df_incorrect = df.loc[df['actual'] != df['predicted']]\n",
    "\n",
    "plt.xlabel('sepal_length'), plt.ylabel('petal_width')\n",
    "\n",
    "# plot training instances (rainbow) + colorbar\n",
    "plt.scatter(x=X_train['sepal_length'].to_pandas(), y=X_train['petal_width'].to_pandas(),\n",
    "            c=y_train.to_pandas(), cmap='rainbow')\n",
    "plt.colorbar()\n",
    "\n",
    "# plot correctly predicted instances (rainbow)\n",
    "plt.scatter(x=df_correct['sepal_length'].to_pandas(), y=df_correct['petal_width'].to_pandas(),\n",
    "            c=df_correct['actual'].to_pandas(), cmap='rainbow')\n",
    "\n",
    "# plot incorrectly predicted instances (black) (except: means 1 or less incorrect)\n",
    "try:\n",
    "    plt.scatter(x=df_incorrect['sepal_length'].to_pandas(), y=df_incorrect['petal_width'].to_pandas(), color='k')\n",
    "except:\n",
    "    plt.scatter(x=df_incorrect['sepal_length'], y=df_incorrect['petal_width'], color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbors\n",
    "Before we wrap up K-Nearest Neighbors, we should get familiar with the Nearest Neighbors algorithm, which enables the query of the K-Nearest Neighbors from a set of input samples. cuML uses [Faiss](https://github.com/facebookresearch/faiss) to run the Nearest Neighbors query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's `.fit()` the `NearestNeighbors` model with our `X_train` data. Note the model is being fed all 4 measurements from each flower, but it is not being told which species any of those flowers belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call `.kneighbors()` to query for the k nearest neighbors of column vectors in `df`. The method returns a tuple with 2 cudf.DataFrames holding the *distances* and *indices* of the k-nearest neighbors for each column vector in X.\n",
    "\n",
    "Let's query for the 3 nearest neighbors of the first 5 points in our testing set (`X_test[:5]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = nn.kneighbors(X_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How far away are the 3 nearest points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At what indices (of `X_train`) can those points be found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the first row of `indices` to classify the 0th flower in `X_test`.\n",
    "\n",
    "species    | target\n",
    "---        | ---\n",
    "setosa     | 0\n",
    "versicolor | 1\n",
    "virginica  | 2\n",
    "\n",
    "This cell prints the neighbor's position of closeness, index of the neighbor and that neighbor's `target` (Iris species)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "votes = []\n",
    "\n",
    "for i in indices.loc[0]:\n",
    "    print(f'neighbor: {n}\\nindex: {i}\\nvote: {y_train.iloc[i]}\\n')\n",
    "    n += 1\n",
    "    votes.append(y_train.iloc[i])\n",
    "    \n",
    "print(f'***\\nClassification: {max(set(votes), key=votes.count)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continued Learning \n",
    "Here are some resources to help fill in any gaps and provide a more complete picture.\n",
    "\n",
    "#### **StatQuest: K-nearest neighbors, Clearly Explained**\n",
    "- Watch on YouTube: [youtu.be/HVXime0nQeI](https://youtu.be/HVXime0nQeI)\n",
    "- Channel: StatQuest with Josh Starmer ([Subscribe](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw?sub_confirmation=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('HVXime0nQeI', width=(1280*0.69), height=(720*0.69))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **_k_-nearest neighbors algorithm**\n",
    "Wikipedia: [https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
    "\n",
    "#### **Nearest Neighbour Algorithm - Part 1**\n",
    "- Watch on YouTube: [youtu.be/JH2IUFmP8JI](https://youtu.be/JH2IUFmP8JI)\n",
    "- Channel:  Darren Barton ([Subscribe](https://www.youtube.com/user/bartondeb1?sub_confirmation=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('JH2IUFmP8JI', width=720, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Nearest neighbour algorithm**\n",
    "Wikipedia: [https://en.wikipedia.org/wiki/Nearest_neighbour_algorithm](https://en.wikipedia.org/wiki/Nearest_neighbour_algorithm) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAPIDS Stable",
   "language": "python",
   "name": "rapids-stable"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
